#funkcijska veza znači da točno možemo izračunati promjenu y uslijed promjene u x
#slučajna komponenta jednaka je 0
#vizualno to prepoznajemo tako što sve točke leže na pravcu ili krivulji

#x je neslučajna varijabla jer ju zadajemo i onda promatramo kako ona utječe na y
#x je egzogen, a y endogen

#funkcija cilja-promatramo udaljenost točkice(stvarna) od pravca(regresijske vrijednosti)
#kvadriramo da poništimo pozitivne i negativne vrijednosti
#kad imamo puno podataka ne moramo brinuti o normalnosti distribucije 

#kad preuzimamo podatke o potrošnji, želimo indeks realnog prometa koji je sezonski i kalendarski prilagođen
#kad pretvaramo bazni indeks u stopu promjene, uspoređujemo vrijednost u 1. mjesecu s vrijednošću u 1. mjesecu prije 12 mjeseci tako da ih podijelimo 
#bazni indeks=(yt-y0)*100
#npr. I13/I1=(y13/y0)*100/(y1/y0)*100, tj. y13/y1-1 


>>>#Primjer 1A:Učitaj podatke iz csv multiTimeline.csv. Podatke prikaži grafički. Komentiraj.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("multiTimeline.csv", index_col=0, parse_dates=True) #kod mene multi_Timeline
print(data)
plt.plot(data, label=data.columns)

plt.xlabel("Mjeseci")
plt.ylabel("Indeks")
plt.title("Google Trends podaci")

plt.legend(loc='upper left', fontsize='small')

plt.show()
plt.close()

#trebamo započeti s podacima od 2012. jer vidimo kaos prije toga vjerojatno zbog toga što podaci nisu bili dobro prikupljeni

data=data.iloc[85:,]

#mislim da 85 jer je 86. redak 01-01-2012

>>>#Primjer 1B:Prikaži grafički podatke pomoću dijagrama rasipanja. Komentiraj.

plt.scatter(data["retail"],data["katalog"])
#plt.scatter(dat["retail"],dat["cijena"])

plt.title("Dijagram rasipanja")

plt.show()
plt.close()

#nema sistematičnog ponašanja, nema veze među podacima

>>>#Primjer 2.: Procijeni model jednostavne linearne regresije gdje je y=retail, a x=cijena. Ucrtaj procijenjeni pravac.

import numpy as np #izračuni
import pandas as pd #učitavanje data framea
import matplotlib.pyplot as plt #vizualizacija
from sklearn.linear_model import LinearRegression #procjene modela, paket: scikit-learn


# Da bi mogli pozvati linearnu regresiju, treba imati numpy objekte

X = data.iloc[:, 0].values.reshape(-1, 1)  #nalazi se u 1. stupcu, što je ovdje 0.
Y = data.iloc[:, 4].values.reshape(-1, 1)  #nalazi se u 5.stupcu, što je ovdje 4.
# -1 znači da python sam prepozna koliko ima redaka, 1 znači da objekt treba imati jedan stupac

#stvara objekt za linearnu regresiju
linear_regressor = LinearRegression()  

linear_regressor.fit(X, Y)  # perform linear regression
Y_pred = linear_regressor.predict(X)  # make predictions

plt.scatter(X, Y)
plt.plot(X, Y_pred, color='red')
plt.show()

>>>#Primjer 3: Ispiši procijenjene parametre te koeficijent determinacije.

#koeficijent determinacije-modelom je objašnjeno % odstupanja
#varijanca regresije, tj. stdev -prosječno odstupanje stvarnih od procijenjenih vrijednosti(RMSE)
#cilj je da nam je odstupanje što manje
#koeficijent determinacije, on je veći što je veći broj varijabli u modelu, zato je bolje koristiti korigirani

from sklearn import metrics

# model evaluation 
mse=metrics.mean_squared_error(Y,Y_pred) 
  
rmse = np.sqrt(mse) 
r2 = metrics.r2_score(Y, Y_pred) 
  
# printing values 
print('Slope:' ,linear_regressor.coef_) 
print('Intercept:', linear_regressor.intercept_) 
print('MSE:',mse) 
print('Root mean squared error: ', rmse) 
print('R2 score: ', r2) 

>>>#Primjer 4.: Prikaži grafikon residual vs. fitted. Komentiraj rezultat.

resid=Y-Y_pred

plt.scatter(resid, Y_pred)

plt.show()

#idealno je da ove točkice budu totalno random, tj. da ne možemo uočiti pravilnosti 
#da imamo heteroskedastičnost, imali bi kao divergenciju 

>>>#Primjer 5: Ispiši procijenjene parametre te koeficijent determinacije u statsmodels paketu.

import statsmodels.api as sm

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

reg_a.summary()

>>>#Primjer 6: Ispiši procijenjene parametre te koeficijent determinacije u statsmodels paketu.

#dodajemo sve google trends podatke u model i gledamo kakve su procjene
#dodajemo sve stupce x-u

X = data.iloc[:, 0:4].values.reshape(-1, 4)  #prva četiri stupca - GT podaci

X0=sm.add_constant(X)
reg_a_full=sm.OLS(Y,X0).fit()

reg_a_full.summary()

#Proučimo objekt reg_a.

type(reg_a)
dir(reg_a)

>>>#Primjer 7: Procijeni četiri modela jednostavne linearne regresije gdje je y=retail, a x=jedna od GT varijabli.

#kad biramo model, prvo pisemo sve moguće kombinacije
#nakon toga gledamo koji ima najveći koeficijent determinacije i taj je najbolji
#2 na k, s tim da je k broj nezavisnih varijabli

#pravila za selekciju
#postepeno uvodimo varijable u model, npr. dodamo prvo x1 -> y x1+x2 y x1+x3 y x1 x4
#gledamo koji ima najveći korigirani R2
#onda tom modelu dodamo x3 i x4, tj. y x1+x2+x3 y x1+x2+x4
#i onda opet od tih modela gledamo koji imaju najveći R2

#može biti i obrnutim redoslijedom tako da prvo uzmemo sve varijable i onda izuzimamo postepeno

#model 1, x=cijena
X = data.iloc[:, 0].values.reshape(-1, 1)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[0],"], beta : ",reg_a.params[1])

#model 2, x=katalog
X = data.iloc[:, 1].values.reshape(-1, 1)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[1],"], beta : ",reg_a.params[1])

#model 3, x=lidl
X = data.iloc[:, 2].values.reshape(-1, 1)  # values converts it into a numpy array

X0=sm.add_constant(X)

reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[2],"], beta : ",reg_a.params[1])

#model 4, x=konzum
X = data.iloc[:, 3].values.reshape(-1, 1)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[3],"], beta : ",reg_a.params[1])

#Najbolji je model 1 jer je koeficijent determinacije najveći. 

#možemo napraviti i for petlju za ovo za kraći način

import statsmodels.api as sm
import pandas as pd

reg_models = []

for i in range(0, 4):
    # Extracting independent variable (feature)
    X = data.iloc[:, i]
    
    # Adding a constant term to the independent variable matrix
    X = sm.add_constant(X)
    
    # Performing Ordinary Least Squares (OLS) regression
    reg_model = sm.OLS(data['retail'], X).fit()
    
    # Append the regression model to the list
    reg_models.append(reg_model)

    # Printing the results for each model
    print(f"Model {i}")
    print("R-Square: ", reg_model.rsquared)
    print("Coefficients:")
    print(reg_model.params)
    print("\n")

>>>#Primjer 8: Predloži način kako bi se odabrao najbolji linearni model od ponuđenih varijabli.

#forward
#1st step - cijena

#2.1 - 2nd step, prva verzija
#y=x1+x2

X = data.iloc[:, [0,1]].values.reshape(-1, 2)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[[0,1]].values,"], beta : ",reg_a.params[1:3])

##########################
#forward
#1st step - cijena

#2.2 - 2nd step, druga verzija
#y=x1+x3

X = data.iloc[:, [0,2]].values.reshape(-1, 2)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[[0,2]].values,"], beta : ",reg_a.params[1:3])

############################
#forward
#1st step - cijena

#2.3 - 2nd step, prva verzija
#y=x1+x4

X = data.iloc[:, [0,3]].values.reshape(-1, 2)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[[0,3]].values,"], beta : ",reg_a.params[1:3])

#gledamo koji od ovih ima najveći koeficijent determinacije
#možemo vidjeti da je to 1. model, tj. x1+x2 pa onda opet idemo dodavati nove varijable tome da vidimo koji bi bio najbolji

####################################
#forward
#1st step - cijena

#2nd step - katalog

#3.1. 3rd step, prva verzija
#y=x1+x2+x3

X = data.iloc[:, [0,1,2]].values.reshape(-1, 3)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared_adj)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[[0,1,2]].values,"], beta : ",reg_a.params[1:3])

######################
#forward
#1st step - cijena

#2nd step - lidl

#3.2. 3rd step, druga verzija
#y=x1+x2+x4

X = data.iloc[:, [0,1,3]].values.reshape(-1, 3)  # values converts it into a numpy array

X0=sm.add_constant(X)
reg_a=sm.OLS(Y,X0).fit()

print("R-Square : ",reg_a.rsquared_adj)
print("GT varijable",data.columns[0:4].values,", beta : ",reg_a_full.params[1:5])
print("GT varijabla[",data.columns[[0,1,3]].values,"], beta : ",reg_a.params[1:3])

#Prema ovim izračunima najbolji bi bio predzadnji model jer ima najveći koeficijent determinacije.
#Međutim, ovi modeli su nastali tako što smo u svaki dodavali više nezavisnih varijabli, što znali da treba gledati korigirani koeficijent determinacije.
#u ovaj kod smo već stavili rsquared da nam računa
#najbolji model:y=x1+x2+x3, tj. retail=cijena+katalog+lidl

#reg_a.rsquared_adj ako hoćemo računati korigirani 













